{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP with disaster tweets using LSTM (tf)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":8.429174,"end_time":"2022-05-13T16:55:02.557852","exception":false,"start_time":"2022-05-13T16:54:54.128678","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:37.130296Z","iopub.execute_input":"2023-02-19T07:58:37.130698Z","iopub.status.idle":"2023-02-19T07:58:40.770794Z","shell.execute_reply.started":"2023-02-19T07:58:37.130616Z","shell.execute_reply":"2023-02-19T07:58:40.769983Z"}}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport plotly.express as px\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nimport re\nimport string\nfrom sklearn.model_selection import train_test_split\nimport nltk\nnltk.download('stopwords')\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nfrom wordcloud import WordCloud","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The problem\nIn this project, we will build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. We will have access to a dataset of 10,000 tweets that were hand classified. Eventually, the LSTM model that we will build will help to solve the problem by classifying the tweets in test dataset into disastrous content or not.","metadata":{}},{"cell_type":"markdown","source":"### Explore the data folder structure","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('../input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-02-19T07:58:40.772502Z","iopub.execute_input":"2023-02-19T07:58:40.773517Z","iopub.status.idle":"2023-02-19T07:58:40.781008Z","shell.execute_reply.started":"2023-02-19T07:58:40.773484Z","shell.execute_reply":"2023-02-19T07:58:40.780193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv(\"../input/nlp-getting-started/train.csv\")","metadata":{"papermill":{"duration":0.087896,"end_time":"2022-05-13T16:55:02.684838","exception":false,"start_time":"2022-05-13T16:55:02.596942","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:40.782027Z","iopub.execute_input":"2023-02-19T07:58:40.782343Z","iopub.status.idle":"2023-02-19T07:58:40.811835Z","shell.execute_reply.started":"2023-02-19T07:58:40.782315Z","shell.execute_reply":"2023-02-19T07:58:40.810923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T07:58:40.812995Z","iopub.execute_input":"2023-02-19T07:58:40.813269Z","iopub.status.idle":"2023-02-19T07:58:40.828555Z","shell.execute_reply.started":"2023-02-19T07:58:40.813244Z","shell.execute_reply":"2023-02-19T07:58:40.827932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.text[:10]","metadata":{"execution":{"iopub.status.busy":"2023-02-19T07:58:40.830584Z","iopub.execute_input":"2023-02-19T07:58:40.831462Z","iopub.status.idle":"2023-02-19T07:58:40.843751Z","shell.execute_reply.started":"2023-02-19T07:58:40.831433Z","shell.execute_reply":"2023-02-19T07:58:40.843059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing\nAs we can find from samples of the text that the tweet content may come with sepecial characters like *#*, *'*, \"=>\" etc. There is also upper case mixed with lower case words together. We would like to clean up the data by stripping unmeaning characters by having the text processed by the pipelines below:","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\ndef lowercase(text):\n    return text.lower()\n\ndef clean(text):\n    t  = re.sub(r\"[^a-zA-Z]\", \" \", text)\n    t  = t.replace(\"https\",\" \")\n    t  = t.replace(\"http\",\" \")\n    return t.translate(str.maketrans(\"\",\"\", string.punctuation))\n\ndef tokenize(text):\n    t = text.split()\n    t = [word for word in t if not word in stop_words]\n    return ' '.join(filter(str.isalpha, t))\n\npreprocessors = [clean, lowercase, tokenize]\n\nfor preprocessor in preprocessors:\n    train['text']= train['text'].apply(preprocessor)","metadata":{"papermill":{"duration":0.052486,"end_time":"2022-05-13T16:55:02.775892","exception":false,"start_time":"2022-05-13T16:55:02.723406","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:40.844680Z","iopub.execute_input":"2023-02-19T07:58:40.845216Z","iopub.status.idle":"2023-02-19T07:58:41.259094Z","shell.execute_reply.started":"2023-02-19T07:58:40.845187Z","shell.execute_reply":"2023-02-19T07:58:41.257866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['count_of_words']=[len(x.split()) for x in train['text'].tolist()]","metadata":{"papermill":{"duration":0.052048,"end_time":"2022-05-13T16:55:03.373204","exception":false,"start_time":"2022-05-13T16:55:03.321156","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:41.260348Z","iopub.execute_input":"2023-02-19T07:58:41.260650Z","iopub.status.idle":"2023-02-19T07:58:41.273625Z","shell.execute_reply.started":"2023-02-19T07:58:41.260622Z","shell.execute_reply":"2023-02-19T07:58:41.272564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-02-19T07:58:41.275327Z","iopub.execute_input":"2023-02-19T07:58:41.275767Z","iopub.status.idle":"2023-02-19T07:58:41.292198Z","shell.execute_reply.started":"2023-02-19T07:58:41.275725Z","shell.execute_reply":"2023-02-19T07:58:41.291354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_counts = train['target'].value_counts()\nprint(unique_counts)\n\nplt.pie(unique_counts.tolist(), \n        labels = ['0-Non-disastrous','1-disastrous'],\n        autopct='%1.2f%%'\n       )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T07:58:41.293253Z","iopub.execute_input":"2023-02-19T07:58:41.293531Z","iopub.status.idle":"2023-02-19T07:58:41.409442Z","shell.execute_reply.started":"2023-02-19T07:58:41.293508Z","shell.execute_reply":"2023-02-19T07:58:41.407959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['count_of_words'].describe()","metadata":{"papermill":{"duration":0.052424,"end_time":"2022-05-13T16:55:03.461412","exception":false,"start_time":"2022-05-13T16:55:03.408988","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:41.410687Z","iopub.execute_input":"2023-02-19T07:58:41.411072Z","iopub.status.idle":"2023-02-19T07:58:41.429416Z","shell.execute_reply.started":"2023-02-19T07:58:41.411017Z","shell.execute_reply":"2023-02-19T07:58:41.428205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\ncount = Counter()\ndef count_word(text):\n    for x in text.values:\n        for word in x.split():\n            count[word]+=1\n    return count\ncounter= count_word(train.text)","metadata":{"papermill":{"duration":0.080643,"end_time":"2022-05-13T16:55:04.013180","exception":false,"start_time":"2022-05-13T16:55:03.932537","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:41.431707Z","iopub.execute_input":"2023-02-19T07:58:41.432575Z","iopub.status.idle":"2023-02-19T07:58:41.496790Z","shell.execute_reply.started":"2023-02-19T07:58:41.432525Z","shell.execute_reply":"2023-02-19T07:58:41.496090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(counter)","metadata":{"papermill":{"duration":0.046381,"end_time":"2022-05-13T16:55:04.097703","exception":false,"start_time":"2022-05-13T16:55:04.051322","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:41.497880Z","iopub.execute_input":"2023-02-19T07:58:41.498289Z","iopub.status.idle":"2023-02-19T07:58:41.504482Z","shell.execute_reply.started":"2023-02-19T07:58:41.498259Z","shell.execute_reply":"2023-02-19T07:58:41.503552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"most = counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:20]:\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y,y=x)","metadata":{"papermill":{"duration":0.048594,"end_time":"2022-05-13T16:55:04.184981","exception":false,"start_time":"2022-05-13T16:55:04.136387","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:41.505660Z","iopub.execute_input":"2023-02-19T07:58:41.505972Z","iopub.status.idle":"2023-02-19T07:58:41.831753Z","shell.execute_reply.started":"2023-02-19T07:58:41.505944Z","shell.execute_reply":"2023-02-19T07:58:41.830824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(' '.join([i[0] for i in most[:50]]))\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T07:58:41.835778Z","iopub.execute_input":"2023-02-19T07:58:41.836290Z","iopub.status.idle":"2023-02-19T07:58:42.018780Z","shell.execute_reply.started":"2023-02-19T07:58:41.836257Z","shell.execute_reply":"2023-02-19T07:58:42.017430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train and validation data\nWe would like split the train data into train and validation datasets based on the 80-20 principal. \nThe train data will be used for model to fit. The validation will be used to verify the model's loss and accuracy. To avoid overfit, data will be shuffled before training.","metadata":{}},{"cell_type":"code","source":"features, targets= train['text'], train['target']\nall_train_features, val_features, all_train_targets, val_targets = train_test_split(\n        features, targets,\n        train_size=0.8,\n        random_state=42,\n        shuffle = True,\n        stratify=targets\n    )","metadata":{"papermill":{"duration":0.053813,"end_time":"2022-05-13T16:55:04.277333","exception":false,"start_time":"2022-05-13T16:55:04.223520","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:42.020805Z","iopub.execute_input":"2023-02-19T07:58:42.021654Z","iopub.status.idle":"2023-02-19T07:58:42.040656Z","shell.execute_reply.started":"2023-02-19T07:58:42.021591Z","shell.execute_reply":"2023-02-19T07:58:42.039476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data= tf.data.Dataset.from_tensor_slices((all_train_features.to_numpy(), all_train_targets.to_numpy())) \nval_data= tf.data.Dataset.from_tensor_slices((val_features.to_numpy(),val_targets.to_numpy()))","metadata":{"papermill":{"duration":0.086544,"end_time":"2022-05-13T16:55:04.402424","exception":false,"start_time":"2022-05-13T16:55:04.315880","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:42.042621Z","iopub.execute_input":"2023-02-19T07:58:42.043465Z","iopub.status.idle":"2023-02-19T07:58:42.066701Z","shell.execute_reply.started":"2023-02-19T07:58:42.043406Z","shell.execute_reply":"2023-02-19T07:58:42.065885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BUFFER_SIZE = 10000\nBATCH_SIZE = 64","metadata":{"papermill":{"duration":0.045375,"end_time":"2022-05-13T16:55:04.783315","exception":false,"start_time":"2022-05-13T16:55:04.737940","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:42.067938Z","iopub.execute_input":"2023-02-19T07:58:42.068510Z","iopub.status.idle":"2023-02-19T07:58:42.074263Z","shell.execute_reply.started":"2023-02-19T07:58:42.068483Z","shell.execute_reply":"2023-02-19T07:58:42.073567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\nval_data = val_data.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","metadata":{"papermill":{"duration":0.052817,"end_time":"2022-05-13T16:55:04.874712","exception":false,"start_time":"2022-05-13T16:55:04.821895","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:42.075525Z","iopub.execute_input":"2023-02-19T07:58:42.076549Z","iopub.status.idle":"2023-02-19T07:58:42.090414Z","shell.execute_reply.started":"2023-02-19T07:58:42.076509Z","shell.execute_reply":"2023-02-19T07:58:42.089777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are not done with the preprocessing yet. A **TextVectorization** layer has basic options for managing text in a Keras model. It transforms a batch of strings into either a list of token indices.\n\nGoing next, we will call this layer's adapt() method on the dataset. When this layer is adapted, it will analyze the dataset, determine the frequency of individual string values, and create a 'vocabulary' from them. ","metadata":{}},{"cell_type":"code","source":"vocab_size= 20000\nmax_len= 15\nvectorize_layer = TextVectorization(\n    max_tokens=vocab_size + 2,\n    split=\"whitespace\",\n    output_mode=\"int\",\n    output_sequence_length=max_len,\n)","metadata":{"papermill":{"duration":0.074759,"end_time":"2022-05-13T16:55:05.097038","exception":false,"start_time":"2022-05-13T16:55:05.022279","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:42.091668Z","iopub.execute_input":"2023-02-19T07:58:42.092240Z","iopub.status.idle":"2023-02-19T07:58:42.108067Z","shell.execute_reply.started":"2023-02-19T07:58:42.092204Z","shell.execute_reply":"2023-02-19T07:58:42.107330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorize_layer.adapt(train_data.map(lambda text, label: text))\nvectorize_layer.adapt(val_data.map(lambda text, label: text))","metadata":{"papermill":{"duration":0.608275,"end_time":"2022-05-13T16:55:05.745792","exception":false,"start_time":"2022-05-13T16:55:05.137517","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:42.109424Z","iopub.execute_input":"2023-02-19T07:58:42.110086Z","iopub.status.idle":"2023-02-19T07:58:42.517262Z","shell.execute_reply.started":"2023-02-19T07:58:42.110034Z","shell.execute_reply":"2023-02-19T07:58:42.516329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = np.array(vectorize_layer.get_vocabulary())\nvocab[:20]","metadata":{"papermill":{"duration":0.063103,"end_time":"2022-05-13T16:55:05.848309","exception":false,"start_time":"2022-05-13T16:55:05.785206","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:42.518575Z","iopub.execute_input":"2023-02-19T07:58:42.519420Z","iopub.status.idle":"2023-02-19T07:58:42.544292Z","shell.execute_reply.started":"2023-02-19T07:58:42.519388Z","shell.execute_reply":"2023-02-19T07:58:42.543270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model architecture\nLSTM will be used for this NLP problem: the first layer is the Embedded layer that uses vectors to represent each word, which will be followed by bidirectional LSTM layer. The main reason for a bidirectional LSTM is that every component of an input sequence has information from both the past and present. For this reason, bidirectional LSTM can produce a more meaningful output, combining LSTM layers from both directions. The next a few layers will be regular dense layers with relu activation. Dropouts are also enabled to mitigate overfitting problem.  Model architecture is summarized below after model construct and is also plotted to provide finer details. The output layer comes with sigmoid activation to address the binary classification problem. ","metadata":{}},{"cell_type":"code","source":"vocab_size= 20000\nmax_len= 15\nmodel = tf.keras.Sequential([\n    vectorize_layer,\n    tf.keras.layers.Embedding(input_dim= vocab_size+1,output_dim=max_len,mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, activation='tanh',return_sequences=True)),\n    tf.keras.layers.Dense(64,activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128,activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128,activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(64,activation='relu'),\n    tf.keras.layers.LSTM(64,return_sequences=True),\n    tf.keras.layers.Dense(64,activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.LSTM(32),\n    tf.keras.layers.Dense(32,activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(16,activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n    ])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","metadata":{"papermill":{"duration":3.328108,"end_time":"2022-05-13T16:55:09.472248","exception":false,"start_time":"2022-05-13T16:55:06.144140","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:42.545355Z","iopub.execute_input":"2023-02-19T07:58:42.545714Z","iopub.status.idle":"2023-02-19T07:58:46.490786Z","shell.execute_reply.started":"2023-02-19T07:58:42.545683Z","shell.execute_reply":"2023-02-19T07:58:46.489725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.summary())","metadata":{"papermill":{"duration":0.058301,"end_time":"2022-05-13T16:55:09.570773","exception":false,"start_time":"2022-05-13T16:55:09.512472","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:46.492258Z","iopub.execute_input":"2023-02-19T07:58:46.492841Z","iopub.status.idle":"2023-02-19T07:58:46.538742Z","shell.execute_reply.started":"2023-02-19T07:58:46.492809Z","shell.execute_reply":"2023-02-19T07:58:46.537791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model,show_shapes=True)","metadata":{"papermill":{"duration":1.173259,"end_time":"2022-05-13T16:55:10.873035","exception":false,"start_time":"2022-05-13T16:55:09.699776","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:46.540163Z","iopub.execute_input":"2023-02-19T07:58:46.540796Z","iopub.status.idle":"2023-02-19T07:58:46.882096Z","shell.execute_reply.started":"2023-02-19T07:58:46.540756Z","shell.execute_reply":"2023-02-19T07:58:46.880907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"histor = model.fit(train_data, epochs=50,\n                    validation_data= val_data)","metadata":{"papermill":{"duration":431.354017,"end_time":"2022-05-13T17:02:22.271635","exception":false,"start_time":"2022-05-13T16:55:10.917618","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T07:58:46.884050Z","iopub.execute_input":"2023-02-19T07:58:46.884406Z","iopub.status.idle":"2023-02-19T08:04:07.229784Z","shell.execute_reply.started":"2023-02-19T07:58:46.884369Z","shell.execute_reply":"2023-02-19T08:04:07.229055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model performance visualization\nWith both train and validation datasets, we will plot Accuracy vs Epochs. We expect with epoch's growth, the accuracy shall go up. On the other hand, the loss will go down when epoch grows.","metadata":{}},{"cell_type":"code","source":"history_dict = histor.history\nhistory_dict.keys()","metadata":{"papermill":{"duration":1.290669,"end_time":"2022-05-13T17:02:24.904662","exception":false,"start_time":"2022-05-13T17:02:23.613993","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T08:04:07.230767Z","iopub.execute_input":"2023-02-19T08:04:07.231061Z","iopub.status.idle":"2023-02-19T08:04:07.239841Z","shell.execute_reply.started":"2023-02-19T08:04:07.231032Z","shell.execute_reply":"2023-02-19T08:04:07.238937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc      = history_dict['accuracy']\nval_accuracy = history_dict['val_accuracy']\nloss     = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'blue', label='Training Loss')\nplt.plot(epochs, val_loss, 'orange', label='Validation Loss')\nplt.title('Loss vs Epoch')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T08:04:07.241239Z","iopub.execute_input":"2023-02-19T08:04:07.241740Z","iopub.status.idle":"2023-02-19T08:04:07.458866Z","shell.execute_reply.started":"2023-02-19T08:04:07.241713Z","shell.execute_reply":"2023-02-19T08:04:07.457827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Training Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.plot(epochs, acc, 'blue', label='Train Accuracy');\nplt.plot(epochs, val_accuracy, 'orange', label='Validation Accuracy' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T08:04:07.460183Z","iopub.execute_input":"2023-02-19T08:04:07.461176Z","iopub.status.idle":"2023-02-19T08:04:07.666055Z","shell.execute_reply.started":"2023-02-19T08:04:07.461136Z","shell.execute_reply":"2023-02-19T08:04:07.664907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction\nWe will use the trained model to make prediction for submission. The text from test dataset must go through the same preprocessing pipeline as how train data went through.","metadata":{}},{"cell_type":"code","source":"test= pd.read_csv(\"../input/nlp-getting-started/test.csv\")\nfor preprocessor in preprocessors:\n    test['text']= test['text'].apply(preprocessor)","metadata":{"papermill":{"duration":1.508841,"end_time":"2022-05-13T17:02:33.246638","exception":false,"start_time":"2022-05-13T17:02:31.737797","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T08:04:07.667669Z","iopub.execute_input":"2023-02-19T08:04:07.668053Z","iopub.status.idle":"2023-02-19T08:04:07.924792Z","shell.execute_reply.started":"2023-02-19T08:04:07.668014Z","shell.execute_reply":"2023-02-19T08:04:07.923516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = test['text']\ntest_dat_array = test_data.to_numpy()\ntest_dat_array = tf.data.Dataset.from_tensor_slices((test_dat_array))\ntest_dat_array = test_dat_array.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\nprediction = model.predict(test_dat_array)","metadata":{"papermill":{"duration":1.271332,"end_time":"2022-05-13T17:02:35.850045","exception":false,"start_time":"2022-05-13T17:02:34.578713","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T08:04:07.926786Z","iopub.execute_input":"2023-02-19T08:04:07.927648Z","iopub.status.idle":"2023-02-19T08:04:14.040155Z","shell.execute_reply.started":"2023-02-19T08:04:07.927586Z","shell.execute_reply":"2023-02-19T08:04:14.039381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_target = []\nfor pred in prediction:\n    if pred >= 0.5:\n        prediction_target.append(1)\n    else: \n        prediction_target.append(0)","metadata":{"papermill":{"duration":1.405305,"end_time":"2022-05-13T17:02:54.851891","exception":false,"start_time":"2022-05-13T17:02:53.446586","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T08:04:14.043658Z","iopub.execute_input":"2023-02-19T08:04:14.045145Z","iopub.status.idle":"2023-02-19T08:04:14.054754Z","shell.execute_reply.started":"2023-02-19T08:04:14.045105Z","shell.execute_reply":"2023-02-19T08:04:14.053906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsubmission","metadata":{"papermill":{"duration":1.296915,"end_time":"2022-05-13T17:02:57.435928","exception":false,"start_time":"2022-05-13T17:02:56.139013","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T08:04:14.055756Z","iopub.execute_input":"2023-02-19T08:04:14.056056Z","iopub.status.idle":"2023-02-19T08:04:14.081263Z","shell.execute_reply.started":"2023-02-19T08:04:14.056030Z","shell.execute_reply":"2023-02-19T08:04:14.080288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['target'] = prediction_target\nsubmission","metadata":{"papermill":{"duration":1.312708,"end_time":"2022-05-13T17:03:00.027135","exception":false,"start_time":"2022-05-13T17:02:58.714427","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T08:04:14.082920Z","iopub.execute_input":"2023-02-19T08:04:14.083658Z","iopub.status.idle":"2023-02-19T08:04:14.095754Z","shell.execute_reply.started":"2023-02-19T08:04:14.083617Z","shell.execute_reply":"2023-02-19T08:04:14.094562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_counts = submission['target'].value_counts()\nprint(unique_counts)\n\nplt.pie(unique_counts.tolist(), \n        labels = ['0-Non-disastrous','1-disastrous'],\n        autopct='%1.2f%%'\n       )\nplt.show()","metadata":{"papermill":{"duration":1.33604,"end_time":"2022-05-13T17:03:02.627227","exception":false,"start_time":"2022-05-13T17:03:01.291187","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T08:04:14.098737Z","iopub.execute_input":"2023-02-19T08:04:14.099009Z","iopub.status.idle":"2023-02-19T08:04:14.177936Z","shell.execute_reply.started":"2023-02-19T08:04:14.098985Z","shell.execute_reply":"2023-02-19T08:04:14.176621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\",index=False)","metadata":{"papermill":{"duration":1.300497,"end_time":"2022-05-13T17:03:13.884890","exception":false,"start_time":"2022-05-13T17:03:12.584393","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T08:04:14.179466Z","iopub.execute_input":"2023-02-19T08:04:14.180090Z","iopub.status.idle":"2023-02-19T08:04:14.191978Z","shell.execute_reply.started":"2023-02-19T08:04:14.180050Z","shell.execute_reply":"2023-02-19T08:04:14.190742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\nBidirectional LSTMs can be used to train two sides, instead of one side of the input sequence. First from left to right on the input sequence and the second in reversed order of the input sequence. It provides one more context to the word to fit in the right context from words coming after and before, this results in faster and fully learning and solving a problem. Due to these characteristics, we built a model by fitting it into cleaned tweets text for training and made prediction for test data.\n\nAs we analyze the performace by looking at the validation accuracy and loss, there are still room to improve, which could be caused by overfitting. Tuning up dropout rates may be a direction to look into as the next step.","metadata":{}}]}